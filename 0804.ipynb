{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization (PPO) 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q) PPO의 objective function을 최적화하는 코드를 아래(Policy class 내부)에 구현해 주세요. 코드에서 물음표 부분을 채워주시면 됩니다 (총 6개의 물음표).\n",
    "\n",
    "PPO에서 policy를 optimize하기 위한 objective function은 다음과 같습니다.\n",
    "\n",
    "$$L(\\theta) = E_t[  min(r_t(\\theta)A_t, clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)A_t) ]$$\n",
    "\n",
    "위 식에서, \n",
    "$$r_t(\\theta) = \\frac{\\pi_\\theta(s_t, a_t)}{\\pi_{old}(s_t, a_t)},$$\n",
    "\n",
    "$$A_t = A(s_t, a_t): advantage function$$\n",
    "에 해당합니다.\n",
    "\n",
    "PPO는 $L(\\theta)$를 최대화하도록 policy $\\pi_\\theta$의 업데이트를 수행하는 알고리즘입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import warnings  \n",
    "with warnings.catch_warnings():  \n",
    "    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "    import tensorflow.compat.v1 as tf\n",
    "    tf.disable_eager_execution()\n",
    "\n",
    "class Policy(object):\n",
    "    \"\"\"\n",
    "    주어진 observation에 대해 수행할 행동의 확률 분포를 Gaussian 분포로 표현하며,\n",
    "    해당 Guassian 분포의 mean과 variance를 출력한다.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, clipping=0.2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            obs_dim: observation의 dimension\n",
    "            act_dim: action의 dimensions\n",
    "            clipping: r_t(\\theta)를 clipping 할 범위 (위에 표기된 objective 수식에서의 epsilon에 해당)\n",
    "        \"\"\"\n",
    "        self.obs_ph = tf.keras.layers.Input(obs_dim, name='obs')\n",
    "        self.act_ph = tf.keras.layers.Input(act_dim, name='act')\n",
    "        self.advantages_ph = tf.keras.layers.Input((None,), name='advantages')\n",
    "\n",
    "        h1 = tf.keras.layers.Dense(32, activation=\"tanh\", name=\"h1\")(self.obs_ph)\n",
    "        h2 = tf.keras.layers.Dense(32, activation=\"tanh\", name=\"h2\")(h1)\n",
    "\n",
    "        # observation이 주어졌을 때, 수행할 행동에 대한 Gaussian 확률 분포의 mean (observation dependent).\n",
    "        self.means = tf.keras.layers.Dense(act_dim, name=\"means\", activation=\"linear\")(h2)\n",
    "\n",
    "        # 수행할 행동의 Gaussian 분포에 대한 전역적 log(variance) 값 (observation independent).\n",
    "        # 참고로, tf.exp(self.log_vars) 를 수행하면 Gaussian 분포의 variance가 됨.\n",
    "        self.log_vars = tf.get_variable('log_vars', (act_dim), tf.float32, tf.constant_initializer(-1.0))\n",
    "\n",
    "        # (observation, action)이 주어졌을 때, 현재 policy가 해당 observation에서 주어진 action을 수행할 확률의 log 값.\n",
    "        log_p = self._log_prob(self.means, self.log_vars, self.act_ph)\n",
    "\n",
    "        # 데이터를 수집한 old policy에서 각 observation에 대한,\n",
    "        # Gaussian 행동 확률 분포의 mean과 log(variance) 입력.\n",
    "        self.old_means_ph = tf.keras.layers.Input(act_dim, name='old_means')\n",
    "        self.old_log_vars_ph = tf.keras.layers.Input(act_dim, name='old_log_vars')\n",
    "\n",
    "        # (observation, action)이 주어졌을 때, 데이터를 수집한 policy(old policy)가\n",
    "        # 해당 observation에서 주어진 action을 수행할 확률의 log 값.\n",
    "        log_p_old = self._log_prob(self.old_means_ph, self.old_log_vars_ph, self.act_ph)\n",
    "\n",
    "        # ===============================================================================================#\n",
    "        # Your code here (아래의 물음표를 채워주세요.)\n",
    "        # ===============================================================================================#\n",
    "        pg_ratio = tf.exp(???? - ????)  # pg_ratio는 PPO objective에서 r_t(\\theta)에 해당.\n",
    "        clipped_pg_ratio = tf.clip_by_value(pg_ratio, 1 - ????, 1 + ????)\n",
    "        surrogate_objective = tf.minimum(????, ????)  # [hint] A_t의 값으로 self.advantages_ph를 활용.\n",
    "        # ===============================================================================================#\n",
    "\n",
    "        self.loss = -tf.reduce_mean(surrogate_objective)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "        self.train_op = optimizer.minimize(self.loss)\n",
    "\n",
    "        # 주어진 Gaussian 분포의 mean과 log(variance)로 action을 샘플.\n",
    "        # action ~ N(mean, variance) 는 action = mean + (starndard_deviation) * e, e ~ N(0, 1) 로 표현 가능.\n",
    "        self.sampled_act = self.means + tf.exp(self.log_vars / 2.0) * tf.random_normal(shape=(act_dim,))\n",
    "\n",
    "        self.epochs = 20  # 주어진 데이터에 대해 현재 policy update를 반복할 epoch 수\n",
    "        self.sess = tf.keras.backend.get_session()\n",
    "\n",
    "    def _log_prob(self, means, log_vars, x):\n",
    "        \"\"\"\n",
    "        Guassian 분포의 mean과 log(variance)가 주어졌을 때, 특정 값 x의 log 확률을 계산하여 리턴.\n",
    "        \"\"\"\n",
    "        log_prob = -0.5 * tf.reduce_sum(log_vars)\n",
    "        log_prob += -0.5 * tf.reduce_sum(tf.square(x - means) / tf.exp(log_vars), axis=1)\n",
    "        return log_prob\n",
    "\n",
    "    def sample(self, obs):\n",
    "        \"\"\"\n",
    "        주어진 각 observation에 대해 수행할 행동을 policy의 Gaussian 확률분포로부터 샘플링하여 리턴.\n",
    "        \"\"\"\n",
    "        feed_dict = {\n",
    "            self.obs_ph: obs\n",
    "        }\n",
    "        return self.sess.run(self.sampled_act, feed_dict=feed_dict)\n",
    "\n",
    "    def update(self, observes, actions, advantages, logger):\n",
    "        \"\"\"\n",
    "        환경에서 수집된 데이터(observes, actions, advantages)를 기반으로 현재 policy의 업데이트를 수행.\n",
    "        \"\"\"\n",
    "        feed_dict = {\n",
    "            self.obs_ph: observes,\n",
    "        }\n",
    "\n",
    "        # 수집된 observation에 대해 데이터 수집 policy (old policy)가 나타내는,\n",
    "        # Gaussian 행동 분포의 mean과 log(variance)를 계산.\n",
    "        old_means, old_log_vars = self.sess.run([self.means, self.log_vars], feed_dict)\n",
    "\n",
    "        feed_dict = {\n",
    "            self.obs_ph: observes,\n",
    "            self.act_ph: actions,\n",
    "            self.advantages_ph: [advantages],\n",
    "            self.old_means_ph: old_means,\n",
    "            self.old_log_vars_ph: [old_log_vars],\n",
    "        }\n",
    "\n",
    "        # clipped surrogate objective를 통해 현재 policy를 update.\n",
    "        for e in range(self.epochs):\n",
    "            self.sess.run(self.train_op, feed_dict)\n",
    "            policy_loss = self.sess.run(self.loss, feed_dict)\n",
    "\n",
    "        logger.log({'PolicyLoss': policy_loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "작성한 Policy class가 잘 동작하는지 여부를 아래의 코드로 테스트해 볼 수 있습니다. 아래는 ContinuousCartPole 환경을 활용하며, 학습중 발생하는 로그 중 Mean Return에 해당하는 값이 에피소드가 증가함에 따라 함께 증가한다면, 학습이 잘 진행되는 것으로 판단할 수 있습니다. 이 환경에서 얻을 수 있는 최대 Mean Return은 1000.0입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from miscs.m0804.train import train\n",
    "\n",
    "env_name = 'ContinuousCartPole-v0'\n",
    "obs_dim = 5\n",
    "act_dim = 1\n",
    "policy = Policy(obs_dim, act_dim, clipping=0.2)\n",
    "train(env_name, policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
