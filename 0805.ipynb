{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "import math\n",
    "import random\n",
    "from random import randint\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from miscs.m0805.bpe import BytePairEncoding\n",
    "\n",
    "\n",
    "def dot_scaled_attention(\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        padding_mask: torch.Tensor\n",
    "):\n",
    "    \"\"\" Dot scaled attention\n",
    "    Implement dot-product scaled attention which takes query, key, value and gives attention scores.\n",
    "    Like assignment 2, <PAD> should not be attended when calculating attention distribution.\n",
    "    Hint: If you get still stuck on the test cases, remind the structure of the Transformer decoder.\n",
    "    In the Transformer decoder, value and key from the encoder have same shape but query does not.\n",
    "    Arguments:\n",
    "    query -- Query tensor\n",
    "                in shape (sequence_length, batch_size, d_k)\n",
    "    key -- Key tensor\n",
    "                in shape (sequence_length, batch_size, d_k)\n",
    "    value -- Value tensor\n",
    "                in shape (sequence_length, batch_size, d_k)\n",
    "    padding_mask -- Padding mask tensor in torch.bool type\n",
    "                in shape (sequence_length, batch_size)\n",
    "                True for <PAD>, False for non-<PAD>\n",
    "    Returns:\n",
    "    attention -- Attention result tensor\n",
    "                in shape (sequence_length, batch_size, d_k)\n",
    "    \"\"\"\n",
    "\n",
    "    # Because we will use only Transformer's encoder, all of input tensors have same shape\n",
    "    assert query.shape == key.shape == value.shape\n",
    "    assert padding_mask.shape == query.shape[:2]\n",
    "    query_shape = query.shape\n",
    "    _, _, d_k = query_shape\n",
    "\n",
    "    # All vlues in last dimension (d_k dimension) are zeros for <PAD> location\n",
    "    assert (padding_mask == (query == 0.).all(-1)).all()\n",
    "    assert (padding_mask == (key == 0.).all(-1)).all()\n",
    "    assert (padding_mask == (value == 0.).all(-1)).all()\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    query = query.permute(None, None, None)\n",
    "    key = key.permute(None, None, None)\n",
    "    value = value.permute(None, None, None)\n",
    "    padding_mask = padding_mask.T.unsqueeze(None)\n",
    "\n",
    "    scores = torch.bmm(None, None) / math.sqrt(None)\n",
    "    scores = scores.masked_fill(None == 1, float('-inf'))\n",
    "    p_attn = nn.Softmax(dim=-1)(None)\n",
    "    attention = torch.bmm(None, None)\n",
    "    # END YOUR CODE\n",
    "\n",
    "    # Don't forget setting attention result of <PAD> to zeros.\n",
    "    # This will be useful for debuging\n",
    "    padding_mask = padding_mask.squeeze(1)\n",
    "    attention[padding_mask, :] = 0.\n",
    "\n",
    "    attention = attention.permute(1, 0, 2)\n",
    "    assert attention.shape == query_shape\n",
    "    return attention\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_dim: int=256,\n",
    "                 n_head: int=8\n",
    "                 ):\n",
    "        \"\"\" Multi-head attention initializer\n",
    "        Use below attributes to implement the forward function\n",
    "        Attributes:\n",
    "        n_head -- the number of heads\n",
    "        d_k -- Hidden dimension of the dot scaled attention\n",
    "        V_linear -- Linear function to project hidden_dim of value to d_k\n",
    "        K_linear -- Linear function to project hidden_dim of key to d_k\n",
    "        Q_linear -- Linear function to project hidden_dim of query to d_k\n",
    "        O_linear -- Linear function to project collections of d_k to hidden_dim\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert hidden_dim % n_head == 0\n",
    "        self.n_head = n_head\n",
    "        self.d_k = hidden_dim // n_head\n",
    "        self.V_linear = nn.Linear(hidden_dim, self.n_head * self.d_k, bias=False)\n",
    "        self.K_linear = nn.Linear(hidden_dim, self.n_head * self.d_k, bias=False)\n",
    "        self.Q_linear = nn.Linear(hidden_dim, self.n_head * self.d_k, bias=False)\n",
    "        self.O_linear = nn.Linear(self.n_head * self.d_k, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self,\n",
    "                value: torch.Tensor,\n",
    "                key: torch.Tensor,\n",
    "                query: torch.Tensor,\n",
    "                padding_mask: torch.Tensor\n",
    "                ):\n",
    "        \"\"\" Multi-head attention forward function\n",
    "        Implement multi-head attention which takes value, key, query, and gives attention score.\n",
    "        Use dot-scaled attention you have implemented above.\n",
    "        Note: If you adjust the dimension of batch_size dynamically,\n",
    "              you can implement this function without any iteration.\n",
    "        Parameters:\n",
    "        value -- Value tensor\n",
    "                    in shape (sequence_length, batch_size, hidden_dim)\n",
    "        key -- Key tensor\n",
    "                    in shape (sequence_length, batch_size, hidden_dim)\n",
    "        query -- Query tensor\n",
    "                    in shape (sequence_length, batch_size, hidden_dim)\n",
    "        padding_mask -- Padding mask tensor in torch.bool type\n",
    "                    in shape (sequence_length, batch_size)\n",
    "                    True for <PAD>, False for non-<PAD>\n",
    "        Returns:\n",
    "        attention -- Attention result tensor\n",
    "                    in shape (sequence_length, batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        assert value.shape == key.shape == query.shape\n",
    "        assert padding_mask.shape == query.shape[:2]\n",
    "        input_shape = value.shape\n",
    "        seq_length, batch_size, hidden_dim = input_shape\n",
    "\n",
    "        queries = self.Q_linear(query)\n",
    "        keys = self.K_linear(key)\n",
    "        values = self.V_linear(value)\n",
    "\n",
    "        queries = queries.reshape(seq_length, self.n_head * batch_size, self.d_k)\n",
    "        keys = keys.reshape(seq_length, self.n_head * batch_size, self.d_k)\n",
    "        values = values.reshape(seq_length, self.n_head * batch_size, self.d_k)\n",
    "\n",
    "        padding_mask = torch.repeat_interleave(padding_mask, self.n_head, dim=1)\n",
    "        attention = dot_scaled_attention(queries, keys, values, padding_mask)\n",
    "        attention = self.O_linear(attention.reshape(seq_length, batch_size, -1))\n",
    "\n",
    "        assert attention.shape == input_shape\n",
    "        return attention\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_dim: int=256,\n",
    "                 dropout: float=.1,\n",
    "                 n_head: int=8,\n",
    "                 feed_forward_dim: int=512\n",
    "                 ):\n",
    "        \"\"\" Transformer Encoder Block initializer\n",
    "        Use below attributes to implement the forward function\n",
    "        Attributes:\n",
    "        attention -- Multi-head attention layer\n",
    "        output -- Output layer\n",
    "        dropout1, dropout2 -- Dropout layers\n",
    "        norm1, norm2 -- Layer normalization layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Attention Layer\n",
    "        self.attention = MultiHeadAttention(hidden_dim, n_head)\n",
    "\n",
    "        # Output Layer\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, feed_forward_dim, bias=True),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(feed_forward_dim, hidden_dim, bias=True)\n",
    "        )\n",
    "\n",
    "        # Dropout Layers\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        # Layer Normalization Layers\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                padding_mask: torch.Tensor\n",
    "                ):\n",
    "        \"\"\"  Transformer Encoder Block forward function\n",
    "        Implement transformer encoder block with the given attributes.\n",
    "        We will stack this module to constuct a BERT model.\n",
    "        Note: Dropout should be applied before adding residual connections\n",
    "        Paramters:\n",
    "        x -- Input tensor\n",
    "                in shape (sequence_length, batch_size, hidden_dim)\n",
    "        padding_mask -- Padding mask tensor in torch.bool type\n",
    "                in shape (sequence_length, batch_size)\n",
    "                True for <PAD>, False for non-<PAD>\n",
    "        Returns:\n",
    "        output -- output tensor\n",
    "                in shape (sequence_length, batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        input_shape = x.shape\n",
    "\n",
    "        # All vlues in last dimension (hidden dimension) are zeros for <PAD> location\n",
    "        assert (padding_mask == (x == 0.).all(-1)).all()\n",
    "\n",
    "        output = self.dropout1(self.attention(x, x, x, padding_mask)) + x\n",
    "        output = self.norm1(output)\n",
    "        output = self.dropout2(self.output(output)) + output\n",
    "        output = self.norm2(output)\n",
    "\n",
    "        # Don't forget setting output result of <PAD> to zeros.\n",
    "        # This will be useful for debuging\n",
    "        output[padding_mask] = 0.\n",
    "\n",
    "        assert output.shape == input_shape\n",
    "        return output\n",
    "\n",
    "#######################################################\n",
    "# Helper functions below. DO NOT MODIFY!              #\n",
    "# Read helper classes to implement trainers properly! #\n",
    "#######################################################\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\" Positional encoder from pyTorch tutorial\n",
    "    This class injects token position information to the tensor\n",
    "    Link: https://pytorch.org/tutorials/beginner/transformer_tutorial\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, hidden_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, hidden_dim, 2).float() * (-math.log(10000.0) / hidden_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe[:, None, :]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), ...]\n",
    "\n",
    "\n",
    "class SegmentationEmbeddings(nn.Module):\n",
    "    \"\"\" Segmentaion embedding layer\n",
    "    This class injects segmentation information to the tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, max_seg_id=3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(max_seg_id, hidden_dim)\n",
    "\n",
    "    def forward(self, x, tokens):\n",
    "        seg_ids = torch.cumsum(tokens == BytePairEncoding.SEP_token_idx, dim=0) \\\n",
    "                  - (tokens == BytePairEncoding.SEP_token_idx).to(torch.long)\n",
    "        return x + self.embedding(seg_ids)\n",
    "\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "    \"\"\" BERT base model\n",
    "    MLM & NSP pretraining model and IMDB classification model share the structure of this class.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, token_num: int,\n",
    "            hidden_dim: int=256, num_layers: int=4,\n",
    "            dropout: float=0.1, max_len: int=1000,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(token_num, hidden_dim, padding_idx=BytePairEncoding.PAD_token_idx)\n",
    "        self.position_encoder = PositionalEncoding(hidden_dim, max_len)\n",
    "        self.segmentation_embedding = SegmentationEmbeddings(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        encoders = [TransformerEncoderBlock(hidden_dim=hidden_dim, dropout=dropout, **kwargs) \\\n",
    "                    for _ in range(num_layers)]\n",
    "        self.encoders = nn.ModuleList(encoders)\n",
    "\n",
    "    def forward(self, x):\n",
    "        padding_mask = x == BytePairEncoding.PAD_token_idx\n",
    "        out = self.embedding(x)\n",
    "        out = self.position_encoder(out)\n",
    "        out = self.segmentation_embedding(out, x)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out[padding_mask] = 0.\n",
    "        for encoder in self.encoders:\n",
    "            out = encoder(out, padding_mask=padding_mask)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MLMandNSPmodel(BaseModel):\n",
    "    \"\"\" MLM & NSP model\n",
    "    Pretraining model for MLM & NSP\n",
    "    \"\"\"\n",
    "    def __init__(self, token_num: int, hidden_dim=256, **kwargs):\n",
    "        super().__init__(token_num, hidden_dim=hidden_dim, **kwargs)\n",
    "        self.MLM_output = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, token_num)\n",
    "        )\n",
    "        self.NSP_output = nn.Linear(hidden_dim, 2) # Binary classes, 0 for False and 1 for True.\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = super().forward(x)\n",
    "        return self.MLM_output(out), self.NSP_output(out[0, ...])\n",
    "\n",
    "\n",
    "class IMDBmodel(BaseModel):\n",
    "    \"\"\" IMDB classification model\n",
    "    IMDB review classification model which generates binary classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, token_num: int, hidden_dim=256, **kwargs):\n",
    "        super().__init__(token_num, hidden_dim=hidden_dim, **kwargs)\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * 2, 2) # Binary classes, 0 for False and 1 for True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = super().forward(x)\n",
    "        return self.output(out[0, ...])\n",
    "\n",
    "#############################################\n",
    "# Testing functions below.                  #\n",
    "#############################################\n",
    "\n",
    "\n",
    "def test_dot_scaled_attention():\n",
    "    print(\"======Dot Scaled Attention Test Case======\")\n",
    "    sequence_length = 10\n",
    "    batch_size = 8\n",
    "    d_k = 3\n",
    "\n",
    "    padding_mask = []\n",
    "    for _ in range(0, batch_size):\n",
    "        padding_length = randint(0, sequence_length // 2)\n",
    "        padding_mask.append([False] * (sequence_length - padding_length) + [True] * padding_length)\n",
    "    padding_mask = torch.Tensor(padding_mask).to(torch.bool).T\n",
    "    x = torch.normal(0, 1, [sequence_length, batch_size, d_k], requires_grad=True)\n",
    "    x.data[padding_mask, :] = 0.\n",
    "\n",
    "    attention = dot_scaled_attention(query=x, key=x, value=x, padding_mask=padding_mask)\n",
    "\n",
    "    # the first test\n",
    "    expected_attn = torch.Tensor([[ 0.12579274, -0.68755102, -0.23434006],\n",
    "                                  [ 0.03455823, -0.43622059, -0.38654214],\n",
    "                                  [ 1.03157961, -1.71973670, -0.77598560]])\n",
    "    assert attention[:3, :3, 0].allclose(expected_attn, atol=1e-7), \\\n",
    "        \"Your attention does not match the expected result\"\n",
    "    print(\"The first test passed!\")\n",
    "\n",
    "    # the second test\n",
    "    (attention ** 2).sum().backward()\n",
    "    expected_grad = torch.Tensor([[ 0.16342157, -0.95254862, -0.46277216],\n",
    "                                  [ 0.06600342, -1.18473446, -1.33262730],\n",
    "                                  [ 3.98706675, -7.13838005, -2.49680638]])\n",
    "    assert x.grad[:3, :3, 0].allclose(expected_grad, atol=1e-7), \\\n",
    "        \"Your gradient does not match the expected result\"\n",
    "    print(\"The second test passed!\")\n",
    "\n",
    "    print(\"All 2 tests passed!\")\n",
    "\n",
    "\n",
    "def test_multi_head_attention():\n",
    "    print(\"======Multi-Head Attention Test Case======\")\n",
    "    sequence_length = 10\n",
    "    batch_size = 8\n",
    "    hidden_dim = 6\n",
    "    n_head = 3\n",
    "\n",
    "    padding_mask = []\n",
    "    for _ in range(0, batch_size):\n",
    "        padding_length = randint(0, sequence_length // 2)\n",
    "        padding_mask.append([False] * (sequence_length - padding_length) + [True] * padding_length)\n",
    "    padding_mask = torch.Tensor(padding_mask).to(torch.bool).T\n",
    "    x = torch.normal(0, 1, [sequence_length, batch_size, hidden_dim], requires_grad=True)\n",
    "    x.data[padding_mask, :] = 0.\n",
    "\n",
    "    layer = MultiHeadAttention(hidden_dim=hidden_dim, n_head=n_head)\n",
    "    attention = layer(value=x, key=x, query=x, padding_mask=padding_mask)\n",
    "\n",
    "    # the first test\n",
    "    expected_attn = torch.Tensor([[ 0.05339770, -0.01818473, -0.08113014],\n",
    "                                  [ 0.02698230,  0.00074039,  0.02418777],\n",
    "                                  [-0.00273581, -0.08840958, -0.04401136]])\n",
    "    assert attention[:3, :3, 0].allclose(expected_attn, atol=1e-7), \\\n",
    "        \"Your attention does not match the expected result\"\n",
    "    print(\"The first test passed!\")\n",
    "\n",
    "    # the second test\n",
    "    (attention ** 2).sum().backward()\n",
    "    expected_grad = torch.Tensor([[ 0.00302105, -0.02852733,  0.01455163],\n",
    "                                  [-0.03849730, -0.03096544, -0.01094508],\n",
    "                                  [-0.08066120, -0.01674306,  0.04139223]])\n",
    "    assert x.grad[:3, :3, 0].allclose(expected_grad, atol=1e-7), \\\n",
    "        \"Your gradient does not match the expected result\"\n",
    "    print(\"The second test passed!\")\n",
    "\n",
    "    print(\"All 2 tests passed!\")\n",
    "\n",
    "\n",
    "def test_transformer_encoder_block():\n",
    "    print(\"======Transformer Encoder Block Test Case======\")\n",
    "    sequence_length = 10\n",
    "    batch_size = 8\n",
    "    hidden_dim = 6\n",
    "    n_head = 3\n",
    "    feed_forward_dim = 12\n",
    "\n",
    "    padding_mask = []\n",
    "    for _ in range(0, batch_size):\n",
    "        padding_length = randint(0, sequence_length // 2)\n",
    "        padding_mask.append([False] * (sequence_length - padding_length) + [True] * padding_length)\n",
    "    padding_mask = torch.Tensor(padding_mask).to(torch.bool).T\n",
    "    x = torch.normal(0, 1, [sequence_length, batch_size, hidden_dim], requires_grad=True)\n",
    "    x.data[padding_mask, :] = 0.\n",
    "\n",
    "    layer = TransformerEncoderBlock(hidden_dim=hidden_dim, n_head=n_head, feed_forward_dim=feed_forward_dim)\n",
    "    encoded = layer(x, padding_mask=padding_mask)\n",
    "\n",
    "    # the test case\n",
    "    expected_value = torch.Tensor([[ 0.17558186,  1.45592594,  0.78872836],\n",
    "                                   [-0.69810724, -0.71614712,  0.25665924],\n",
    "                                   [-2.15096855,  1.82032120,  1.11762428]])\n",
    "    assert encoded[:3, :3, 0].allclose(expected_value, atol=1e-7), \\\n",
    "        \"Your encoded value does not match the expected result\"\n",
    "    print(\"The test case passed!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.set_printoptions(precision=8)\n",
    "    random.seed(1234)\n",
    "    torch.manual_seed(1234)\n",
    "\n",
    "    test_dot_scaled_attention()\n",
    "    test_multi_head_attention()\n",
    "    test_transformer_encoder_block()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
