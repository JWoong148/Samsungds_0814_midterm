{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1) Image retrival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lOLsO5_FhfDv"
   },
   "source": [
    "## Image Retrieval 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4654,
     "status": "ok",
     "timestamp": 1595824037355,
     "user": {
      "displayName": "‍이정수[ 학부졸업 / 산업경영공학부 ]",
      "photoUrl": "",
      "userId": "12549429419312222565"
     },
     "user_tz": -540
    },
    "id": "6HuVSgv1hfDw",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7ea95e0c1dfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mLA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from skimage.transform import resize\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H2YwM6V9hfDy"
   },
   "source": [
    "\n",
    "## Feature Extractor (Color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r84uG-czhfDz"
   },
   "outputs": [],
   "source": [
    "#  extract 3D HSV color histogram from images\n",
    "class Color_Extractor:\n",
    "    def __init__(self, bins):\n",
    "        # store # of bins for histogram\n",
    "        self.bins = bins\n",
    "\n",
    "    # convert RGB to HSV and \n",
    "    # initialize features to quantify and represent the image\n",
    "    def describe(self, image):\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "        features = []\n",
    "\n",
    "        # grab dimensions and computer center of image\n",
    "        # from beginning 0 to end-1 = 1, i.e. shape[0] & shape[1]\n",
    "        (h, w) = image.shape[:2]\n",
    "        (cx, cy) = (int(w * 0.5), int(h * 0.5))\n",
    "\n",
    "        # divide image into top-left, top-right, bottom-right, bottom-left corner segments as mask\n",
    "        segments = [(0,cx,0,cy), (0,cx,cy,h), (cx,w,cy,h), (cx,w,0,cy)]\n",
    "\n",
    "        # construct an elliptical mask representing the center of the image\n",
    "        (axesX, axesY) = (int(w * 0.75 / 2), int(h * 0.75 / 2))\n",
    "        ellipse_mask = np.zeros(image.shape[:2], dtype = \"uint8\")\n",
    "        cv2.ellipse(ellipse_mask, (cx, cy), (axesX, axesY), 0, 0, 360, (255, 255, 255), -1)\n",
    "\n",
    "        # loop over mask corners\n",
    "        for seg in segments:\n",
    "            # construct mask for each corner by np.zeros()\n",
    "            corner_mask = np.zeros(image.shape[:2], dtype = 'uint8')\n",
    "            # draw rectangle mask on corner_mask object\n",
    "            corner_mask[seg[0]:seg[1], seg[2]:seg[3]] = 255\n",
    "            corner_mask = cv2.subtract(corner_mask, ellipse_mask)\n",
    "\n",
    "            # extract hsv histogram\tfrom segment of image with mask\t\n",
    "            hist = self.histogram(image, corner_mask)\n",
    "\n",
    "            # update feature vector\n",
    "            features.extend(hist)\n",
    "\n",
    "        # extract hsv histogram from ellipse with mask\n",
    "        hist_ellipse = self.histogram(image, ellipse_mask)\n",
    "        features.extend(hist_ellipse)\n",
    "\n",
    "        return ???\n",
    "    \n",
    "    # Calculate the histogram of the masked region of the image\n",
    "    def histogram(self,image, mask):\n",
    "        # use number of bins per channel; \n",
    "        hist = cv2.calcHist([image], [0,1,2], mask, self.bins, [0, 256, 0, 256, 0, 256])\n",
    "        \n",
    "        # normalize histogram to obtain scale invariance\n",
    "        hist = cv2.normalize(hist, None).flatten()\n",
    "    \n",
    "        return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DvZZ3fVwhfD1"
   },
   "outputs": [],
   "source": [
    "def extract_color_features(path):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create a list to append feature vectors to\n",
    "    features = []\n",
    "    list_imgs_names = os.listdir(path)\n",
    "    color_extractor = Color_Extractor((8, 8, 8))\n",
    "    image_all = []\n",
    "    \n",
    "    # Iterate through the list of images\n",
    "    for img_name in list_imgs_names:        \n",
    "        # Read in each one by one\n",
    "        img_path = os.path.join(path, img_name)\n",
    "        image = mpimg.imread(img_path) # Read the images\n",
    "        image = ???(image, (224, 224), mode='constant') # Resize the images\n",
    "        image = (image * 255).astype(np.uint8)\n",
    "        feature_image = np.copy(image)\n",
    "        \n",
    "        feature = np.array(color_extractor.describe(feature_image))\n",
    "        feature = feature / LA.norm(???) # Feature Normalization\n",
    "        features.append(feature)\n",
    "        image_all.append(img_name)\n",
    "    \n",
    "    time_elapsed = ???\n",
    "    \n",
    "    print('Feature extraction complete in {:.02f}s'.format(time_elapsed % 60))\n",
    "    \n",
    "    # Return list of feature vectors\n",
    "    return np.array(features), image_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mQOk0dk7hfD3"
   },
   "outputs": [],
   "source": [
    "def test_color_features():\n",
    "    print('Extract features from data')\n",
    "    path = 'miscs/m0807/data'\n",
    "    feats, image_list = extract_color_features(path)\n",
    "\n",
    "    print('Extract features from query image')\n",
    "    test = 'miscs/m0807/test'\n",
    "    feat_single, image = extract_color_features(test)\n",
    "    \n",
    "    # Calculate the scores\n",
    "    scores  = ???(feat_single, feats.T)  # 스코어 구하기\n",
    "    sort_ind = ???(scores)[0][::-1]      # 스코어 정렬하기\n",
    "    scores = scores[0, sort_ind]\n",
    "\n",
    "    # Show the results\n",
    "    maxres = 10\n",
    "    imlist = [image_list[index] for i, index in enumerate(sort_ind[0:maxres])]\n",
    "    print (\"top %d images in order are: \" %maxres, imlist)\n",
    "\n",
    "    fig=plt.figure(figsize=(16, 10))\n",
    "    for i in range(len(imlist)):\n",
    "        sample = imlist[i]\n",
    "        img = mpimg.imread('miscs/m0807/data' + '/' + sample)\n",
    "        ax = fig.add_subplot(2, 5, i+1)\n",
    "        ax.autoscale()\n",
    "        plt.tight_layout()\n",
    "        plt.imshow(img, interpolation='nearest')\n",
    "        ax.set_title('{:.3f}%'.format(scores[i]))\n",
    "        ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rshd7EDjhfD5",
    "outputId": "2668eb82-65e8-4b7d-c4d6-2cef72ba6fce",
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_color_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ywbwguSuhfED"
   },
   "source": [
    "## Feature Extractor (VGG19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iQJIzgqthfEE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VGG19(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG19, self).__init__()\n",
    "        self.vgg19 = torchvision.models.vgg19(pretrained = True) # vgg 19 model is imported\n",
    "        #print(vgg19)\n",
    "        self.vgg19.classifier = self.vgg19.classifier[0:4]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.vgg19(x)\n",
    "        return out\n",
    "\n",
    "# Set our model with pre-trained model \n",
    "vgg19 = VGG19().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_bTedPL6hfEF"
   },
   "source": [
    "## Feature Extractor (ResNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tE5ssg-4hfEG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the network to extract the features\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        resnet = torchvision.models.resnet50(pretrained = True) # resnet 50 model is imported\n",
    "        \n",
    "        #print(resnet)\n",
    "        self.conv1 = resnet.conv1\n",
    "        self.bn1 = resnet.bn1\n",
    "        self.maxpool = resnet.maxpool\n",
    "        self.layer1 = resnet.layer1\n",
    "        self.layer2 = resnet.layer2\n",
    "        self.layer3 = resnet.layer3\n",
    "        self.layer4 = resnet.layer4\n",
    "\n",
    "    def forward(self, x):\n",
    "        ???\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        ???\n",
    "        out = F.avg_pool2d(out, kernel_size=7, stride=7)\n",
    "\n",
    "        return out\n",
    "\n",
    "# Set our model with pre-trained model \n",
    "resnet = ResNet().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aH9zfPRyhfEI"
   },
   "outputs": [],
   "source": [
    "# Extract ConvNet Features (VGG19, ResNet)\n",
    "def extract_deep_features(path, feature_extractor, feature_size):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    list_imgs_names = os.listdir(path) #list_imgs_names\n",
    "    N = len(list_imgs_names)\n",
    "    feature_all = np.zeros((N, feature_size)) # create an array to store features\n",
    "    image_all = [] # define empy array to store image names\n",
    "    \n",
    "    transform = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    # extract features \n",
    "    for index, img_name in enumerate(list_imgs_names):\n",
    "        img_path = os.path.join(path, img_name)\n",
    "        \n",
    "        # Image Read & Resize\n",
    "        image_np = Image.open(img_path) # Read the images\n",
    "        image_np = np.array(image_np)\n",
    "        image_np = resize(image_np, (224, 224), mode='constant') # Resize the images\n",
    "        image_np = torch.from_numpy(image_np).permute(2, 0, 1).float()\n",
    "        image_np = transform(image_np)\n",
    "        image_np = Variable(image_np.unsqueeze(0))   #bs, c, h, w\n",
    "        image_np = image_np.cuda()\n",
    "        \n",
    "        # Extract Feature\n",
    "        feature = feature_extractor(image_np)\n",
    "        feature = feature.squeeze().cpu().data.numpy()\n",
    "        feature = feature.reshape((1, feature_size)) # Feature Flatten\n",
    "        feature = feature / LA.norm(feature) # Feature Normalization\n",
    "        feature_all[index] = feature\n",
    "        image_all.append(img_name)\n",
    "\n",
    "    time_elapsed = time.time() - start_time\n",
    "\n",
    "    print('Feature extraction complete in {:.02f}s'.format(time_elapsed % 60))\n",
    "\n",
    "    return feature_all, image_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zn0D046DhfEJ"
   },
   "outputs": [],
   "source": [
    "def test_deep_feature(feature_extractor, feature_size):\n",
    "    # Extract features from the dataset\n",
    "    print('Extract features from data')\n",
    "    path = 'miscs/m0807/data'\n",
    "    feats, image_list = extract_deep_features(path, feature_extractor, feature_size=feature_size)\n",
    "\n",
    "    # test image path\n",
    "    print('Extract features from query image')\n",
    "    test = 'miscs/m0807/test'\n",
    "    feat_single, image = extract_deep_features(test, feature_extractor, feature_size=feature_size)\n",
    "    \n",
    "    # Calculate the scores\n",
    "    scores  = np.dot(feat_single, feats.T)\n",
    "    sort_ind = np.argsort(scores)[0][::-1] # sort the scores\n",
    "    scores = scores[0, sort_ind]\n",
    "\n",
    "    # Show the results\n",
    "    maxres = 10\n",
    "    imlist = [image_list[index] for i, index in enumerate(sort_ind[0:maxres])]\n",
    "    print (\"top %d images in order are: \" %maxres, imlist)\n",
    "\n",
    "    fig=plt.figure(figsize=(16, 10))\n",
    "    for i in range(len(imlist)):\n",
    "        sample = imlist[i]\n",
    "        img = mpimg.imread('miscs/m0807/data' + '/' + sample)\n",
    "        ax = fig.add_subplot(2, 5, i+1)\n",
    "        ax.autoscale()\n",
    "        plt.tight_layout()\n",
    "        plt.imshow(img, interpolation='nearest')\n",
    "        ax.set_title('{:.3f}%'.format(scores[i]))\n",
    "        ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cct12xszhfEL",
    "outputId": "c33f0e88-97b1-45fc-bdc2-3ae27fc00380",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# VGG19 Image Retrieval Results\n",
    "test_deep_feature(vgg19, feature_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hsxNA8rUhfEN",
    "outputId": "71afbd9f-6bc3-4073-94c0-097cb597d6d6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ResNet50 Image Retrieval Results\n",
    "test_deep_feature(resnet, feature_size=2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2) Face Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Detection 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "from miscs.m0807.box_utils import nms, calibrate_box, get_image_boxes, convert_to_square, _preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MTCNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MTCNN1](miscs/m0807/image/MTCNN1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MTCNN2](miscs/m0807/image/MTCNN2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: a float tensor with shape [batch_size, c, h, w].\n",
    "        Returns:\n",
    "            a float tensor with shape [batch_size, c*h*w].\n",
    "        \"\"\"\n",
    "\n",
    "        # without this pretrained model isn't working\n",
    "        x = x.transpose(3, 2).contiguous()\n",
    "\n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PNet, self).__init__()\n",
    "        # suppose we have input with size HxW, then\n",
    "        # after first layer: H - 2,\n",
    "        # after pool: ceil((H - 2)/2),\n",
    "        # after second conv: ceil((H - 2)/2) - 2,\n",
    "        # after last conv: ceil((H - 2)/2) - 4,\n",
    "        # and the same for W\n",
    "\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(3, ??, ??, 1)),\n",
    "            ('prelu1', nn.PReLU(??)),\n",
    "            ('pool1', nn.MaxPool2d(2, 2, ceil_mode=True)),\n",
    "\n",
    "            ('conv2', nn.Conv2d(??, ??, ??, 1)),\n",
    "            ('prelu2', nn.PReLU(??)),\n",
    "\n",
    "            ('conv3', nn.Conv2d(??, ??, ??, 1)),\n",
    "            ('prelu3', nn.PReLU(??))\n",
    "        ]))\n",
    "\n",
    "        self.conv4_1 = nn.Conv2d(32, 2, 1, 1)\n",
    "        self.conv4_2 = nn.Conv2d(32, 4, 1, 1)\n",
    "\n",
    "        weights = np.load('miscs/m0807/weights/pnet.npy', allow_pickle=True)[()]\n",
    "        for n, p in self.named_parameters():\n",
    "            p.data = torch.FloatTensor(weights[n])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: a float tensor with shape [batch_size, 3, h, w].\n",
    "        Returns:\n",
    "            b: a float tensor with shape [batch_size, 4, h', w'].\n",
    "            a: a float tensor with shape [batch_size, 2, h', w'].\n",
    "        \"\"\"\n",
    "        x = self.features(x)\n",
    "        a = self.conv4_1(x)\n",
    "        b = self.conv4_2(x) # Bounding Box Regression\n",
    "        a = F.softmax(a, dim=1) # Face Classification\n",
    "        return b, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNet, self).__init__()\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(3, ??, ??, 1)),\n",
    "            ('prelu1', nn.PReLU(??)),\n",
    "            ('pool1', nn.MaxPool2d(3, 2, ceil_mode=True)),\n",
    "\n",
    "            ('conv2', nn.Conv2d(??, ??, 3, 1)),\n",
    "            ('prelu2', nn.PReLU(??)),\n",
    "            ('pool2', nn.MaxPool2d(3, 2, ceil_mode=True)),\n",
    "\n",
    "            ('conv3', nn.Conv2d(??, ??, 2, 1)),\n",
    "            ('prelu3', nn.PReLU(64)),\n",
    "\n",
    "            ('flatten', Flatten()),\n",
    "            ('conv4', nn.Linear(576, 128)),\n",
    "            ('prelu4', nn.PReLU(128))\n",
    "        ]))\n",
    "\n",
    "        self.conv5_1 = nn.Linear(128, 2)\n",
    "        self.conv5_2 = nn.Linear(128, 4)\n",
    "\n",
    "        weights = np.load('miscs/m0807/weights/rnet.npy', allow_pickle=True)[()]\n",
    "        for n, p in self.named_parameters():\n",
    "            p.data = torch.FloatTensor(weights[n])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: a float tensor with shape [batch_size, 3, h, w].\n",
    "        Returns:\n",
    "            b: a float tensor with shape [batch_size, 4].\n",
    "            a: a float tensor with shape [batch_size, 2].\n",
    "        \"\"\"\n",
    "        x = self.features(x)\n",
    "        a = self.conv5_1(x) \n",
    "        b = self.conv5_2(x) # Bounding Box Regression\n",
    "        a = F.softmax(a, dim=1) # Face Classification\n",
    "        return b, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ONet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ONet, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(3, 32, 3, 1)),\n",
    "            ('prelu1', nn.PReLU(32)),\n",
    "            ('pool1', nn.MaxPool2d(3, 2, ceil_mode=True)),\n",
    "\n",
    "            ('conv2', nn.Conv2d(32, 64, 3, 1)),\n",
    "            ('prelu2', nn.PReLU(64)),\n",
    "            ('pool2', nn.MaxPool2d(3, 2, ceil_mode=True)),\n",
    "\n",
    "            ('conv3', nn.Conv2d(64, 64, 3, 1)),\n",
    "            ('prelu3', nn.PReLU(64)),\n",
    "            ('pool3', nn.MaxPool2d(2, 2, ceil_mode=True)),\n",
    "\n",
    "            ('conv4', nn.Conv2d(64, 128, 2, 1)),\n",
    "            ('prelu4', nn.PReLU(128)),\n",
    "\n",
    "            ('flatten', Flatten()),\n",
    "            ('conv5', nn.Linear(1152, 256)),\n",
    "            ('drop5', nn.Dropout(0.25)),\n",
    "            ('prelu5', nn.PReLU(256)),\n",
    "        ]))\n",
    "\n",
    "        self.conv6_1 = nn.Linear(256, ??)\n",
    "        self.conv6_2 = nn.Linear(256, ??)\n",
    "        self.conv6_3 = nn.Linear(256, ??)\n",
    "\n",
    "        weights = np.load('miscs/m0807/weights/onet.npy', allow_pickle=True)[()]\n",
    "        for n, p in self.named_parameters():\n",
    "            p.data = torch.FloatTensor(weights[n])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: a float tensor with shape [batch_size, 3, h, w].\n",
    "        Returns:\n",
    "            c: a float tensor with shape [batch_size, 10].\n",
    "            b: a float tensor with shape [batch_size, 4].\n",
    "            a: a float tensor with shape [batch_size, 2].\n",
    "        \"\"\"\n",
    "        x = self.features(x)\n",
    "        a = self.conv6_1(x)\n",
    "        b = self.conv6_2(x) # Bounding Box Regression\n",
    "        c = self.conv6_3(x) # Face Landmark Detection\n",
    "        a = F.softmax(a, dim=1) # Face Classification\n",
    "        return c, b, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run P-Net, Generate Bounding boxes and do NMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_first_stage(image, net, scale, threshold):\n",
    "    \"\"\"Run P-Net, generate bounding boxes, and do NMS.\n",
    "\n",
    "    Arguments:\n",
    "        image: an instance of PIL.Image.\n",
    "        net: an instance of pytorch's nn.Module, P-Net.\n",
    "        scale: a float number,\n",
    "            scale width and height of the image by this number.\n",
    "        threshold: a float number,\n",
    "            threshold on the probability of a face when generating\n",
    "            bounding boxes from predictions of the net.\n",
    "\n",
    "    Returns:\n",
    "        a float numpy array of shape [n_boxes, 9],\n",
    "            bounding boxes with scores and offsets (4 + 1 + 4).\n",
    "    \"\"\"\n",
    "\n",
    "    # scale the image and convert it to a float array\n",
    "    width, height = image.size\n",
    "    sw, sh = math.ceil(width*scale), math.ceil(height*scale)\n",
    "    img = image.resize((sw, sh), Image.BILINEAR)\n",
    "    img = np.asarray(img, 'float32')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        img = Variable(torch.FloatTensor(_preprocess(img)))\n",
    "        if torch.cuda.is_available():\n",
    "            img = img.cuda()\n",
    "            net = net.cuda()\n",
    "    output = net(img)\n",
    "    probs = output[1].data.cpu().numpy()[0, 1, :, :]\n",
    "    offsets = output[0].data.cpu().numpy()\n",
    "    # probs: probability of a face at each sliding window\n",
    "    # offsets: transformations to true bounding boxes\n",
    "\n",
    "    boxes = _generate_bboxes(probs, offsets, scale, threshold)\n",
    "    if len(boxes) == 0:\n",
    "        return None\n",
    "\n",
    "    keep = nms(boxes[:, 0:5], overlap_threshold=0.5)\n",
    "    return boxes[keep]\n",
    "\n",
    "\n",
    "def _generate_bboxes(probs, offsets, scale, threshold):\n",
    "    \"\"\"Generate bounding boxes at places\n",
    "    where there is probably a face.\n",
    "\n",
    "    Arguments:\n",
    "        probs: a float numpy array of shape [n, m].\n",
    "        offsets: a float numpy array of shape [1, 4, n, m].\n",
    "        scale: a float number,\n",
    "            width and height of the image were scaled by this number.\n",
    "        threshold: a float number.\n",
    "\n",
    "    Returns:\n",
    "        a float numpy array of shape [n_boxes, 9]\n",
    "    \"\"\"\n",
    "\n",
    "    # applying P-Net is equivalent, in some sense, to\n",
    "    # moving 12x12 window with stride 2\n",
    "    stride = 2\n",
    "    cell_size = 12\n",
    "\n",
    "    # indices of boxes where there is probably a face\n",
    "    inds = np.where(probs > threshold)\n",
    "\n",
    "    if inds[0].size == 0:\n",
    "        return np.array([])\n",
    "\n",
    "    # transformations of bounding boxes\n",
    "    tx1, ty1, tx2, ty2 = [offsets[0, i, inds[0], inds[1]] for i in range(4)]\n",
    "    # they are defined as:\n",
    "    # w = x2 - x1 + 1\n",
    "    # h = y2 - y1 + 1\n",
    "    # x1_true = x1 + tx1*w\n",
    "    # x2_true = x2 + tx2*w\n",
    "    # y1_true = y1 + ty1*h\n",
    "    # y2_true = y2 + ty2*h\n",
    "\n",
    "    offsets = np.array([tx1, ty1, tx2, ty2])\n",
    "    score = probs[inds[0], inds[1]]\n",
    "\n",
    "    # P-Net is applied to scaled images\n",
    "    # so we need to rescale bounding boxes back\n",
    "    bounding_boxes = np.vstack([\n",
    "        np.round((stride*inds[1] + 1.0)/scale),\n",
    "        np.round((stride*inds[0] + 1.0)/scale),\n",
    "        np.round((stride*inds[1] + 1.0 + cell_size)/scale),\n",
    "        np.round((stride*inds[0] + 1.0 + cell_size)/scale),\n",
    "        score, offsets\n",
    "    ])\n",
    "    # why one is added?\n",
    "\n",
    "    return bounding_boxes.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces(image, min_face_size=20.0,\n",
    "                 thresholds=[0.6, 0.7, 0.8],\n",
    "                 nms_thresholds=[0.7, 0.7, 0.7]):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        image: an instance of PIL.Image.\n",
    "        min_face_size: a float number.\n",
    "        thresholds: a list of length 3.\n",
    "        nms_thresholds: a list of length 3.\n",
    "\n",
    "    Returns:\n",
    "        two float numpy arrays of shapes [n_boxes, 4] and [n_boxes, 10],\n",
    "        bounding boxes and facial landmarks.\n",
    "    \"\"\"\n",
    "\n",
    "    # LOAD MODELS\n",
    "    pnet = PNet()\n",
    "    rnet = RNet()\n",
    "    onet = ONet()\n",
    "    onet.eval()\n",
    "\n",
    "    # BUILD AN IMAGE PYRAMID\n",
    "    width, height = image.size\n",
    "    min_length = min(height, width)\n",
    "\n",
    "    min_detection_size = 12\n",
    "    factor = 0.707  # sqrt(0.5)\n",
    "\n",
    "    # scales for scaling the image\n",
    "    scales = []\n",
    "\n",
    "    # scales the image so that\n",
    "    # minimum size that we can detect equals to\n",
    "    # minimum face size that we want to detect\n",
    "    m = min_detection_size/min_face_size\n",
    "    min_length *= m\n",
    "\n",
    "    factor_count = 0\n",
    "    # scales = [0.6, 0.42, 0.30, 0.21, 0.15, 0.10, 0.07, 0.05, 0.03]\n",
    "    while min_length > min_detection_size:\n",
    "        scales.append(m*factor**factor_count)\n",
    "        min_length *= factor\n",
    "        factor_count += 1\n",
    "\n",
    "    # STAGE 1\n",
    "\n",
    "    # it will be returned\n",
    "    bounding_boxes = []\n",
    "\n",
    "    # run P-Net on different scales\n",
    "    for s in scales:\n",
    "        boxes = run_first_stage(image, pnet, scale=s, threshold=thresholds[0])\n",
    "        bounding_boxes.append(boxes)\n",
    "\n",
    "    # collect boxes (and offsets, and scores) from different scales\n",
    "    bounding_boxes = [i for i in bounding_boxes if i is not None]\n",
    "    bounding_boxes = np.vstack(bounding_boxes)\n",
    "\n",
    "    keep = nms(bounding_boxes[:, 0:5], nms_thresholds[0]) # NMS (Non-Maximum-Suppression)\n",
    "    bounding_boxes = bounding_boxes[keep]\n",
    "\n",
    "    # use offsets predicted by pnet to transform bounding boxes\n",
    "    bounding_boxes = calibrate_box(bounding_boxes[:, 0:5], bounding_boxes[:, 5:])\n",
    "    # shape [n_boxes, 5]\n",
    "\n",
    "    bounding_boxes = convert_to_square(bounding_boxes)\n",
    "    bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\n",
    "\n",
    "    # STAGE 2\n",
    "\n",
    "    img_boxes = get_image_boxes(bounding_boxes, image, size=24)\n",
    "    with torch.no_grad():\n",
    "        img_boxes = Variable(torch.FloatTensor(img_boxes))\n",
    "        if torch.cuda.is_available():\n",
    "            rnet = rnet.cuda()\n",
    "            img_boxes = img_boxes.cuda()\n",
    "    output = rnet(img_boxes)\n",
    "    offsets = output[0].data.cpu().numpy()  # shape [n_boxes, 4]\n",
    "    probs = output[1].data.cpu().numpy()  # shape [n_boxes, 2]\n",
    "\n",
    "    keep = np.where(probs[:, 1] > thresholds[1])[0]\n",
    "    bounding_boxes = bounding_boxes[keep]\n",
    "    bounding_boxes[:, 4] = probs[keep, 1].reshape((-1,))\n",
    "    offsets = offsets[keep]\n",
    "\n",
    "    keep = nms(bounding_boxes, nms_thresholds[1]) # NMS (Non-Maximum-Suppression)\n",
    "    bounding_boxes = bounding_boxes[keep]\n",
    "    bounding_boxes = calibrate_box(bounding_boxes, offsets[keep])\n",
    "    bounding_boxes = convert_to_square(bounding_boxes)\n",
    "    bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\n",
    "\n",
    "    # STAGE 3\n",
    "\n",
    "    img_boxes = get_image_boxes(bounding_boxes, image, size=48)\n",
    "    if len(img_boxes) == 0:\n",
    "        return [], []\n",
    "    with torch.no_grad():\n",
    "        img_boxes = Variable(torch.FloatTensor(img_boxes))\n",
    "        if torch.cuda.is_available():\n",
    "            onet = onet.cuda()\n",
    "            img_boxes = img_boxes.cuda()\n",
    "    output = onet(img_boxes)\n",
    "    landmarks = output[0].data.cpu().numpy()  # shape [n_boxes, 10]\n",
    "    offsets = output[1].data.cpu().numpy()  # shape [n_boxes, 4]\n",
    "    probs = output[2].data.cpu().numpy()  # shape [n_boxes, 2]\n",
    "\n",
    "    keep = np.where(probs[:, 1] > thresholds[2])[0]\n",
    "    bounding_boxes = bounding_boxes[keep]\n",
    "    bounding_boxes[:, 4] = probs[keep, 1].reshape((-1,))\n",
    "    offsets = offsets[keep]\n",
    "    landmarks = landmarks[keep]\n",
    "\n",
    "    # compute landmark points\n",
    "    width = bounding_boxes[:, 2] - bounding_boxes[:, 0] + 1.0\n",
    "    height = bounding_boxes[:, 3] - bounding_boxes[:, 1] + 1.0\n",
    "    xmin, ymin = bounding_boxes[:, 0], bounding_boxes[:, 1]\n",
    "    landmarks[:, 0:5] = np.expand_dims(xmin, 1) + np.expand_dims(width, 1)*landmarks[:, 0:5]\n",
    "    landmarks[:, 5:10] = np.expand_dims(ymin, 1) + np.expand_dims(height, 1)*landmarks[:, 5:10]\n",
    "\n",
    "    bounding_boxes = calibrate_box(bounding_boxes, offsets)\n",
    "    keep = nms(bounding_boxes, nms_thresholds[2], mode='min') # NMS (Non-Maximum-Suppression)\n",
    "    bounding_boxes = bounding_boxes[keep]\n",
    "    landmarks = landmarks[keep]\n",
    "\n",
    "    return bounding_boxes, landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_bboxes(img, bounding_boxes, facial_landmarks=[]):\n",
    "    \"\"\"Draw bounding boxes and facial landmarks.\n",
    "\n",
    "    Arguments:\n",
    "        img: an instance of PIL.Image.\n",
    "        bounding_boxes: a float numpy array of shape [n, 5].\n",
    "        facial_landmarks: a float numpy array of shape [n, 10].\n",
    "\n",
    "    Returns:\n",
    "        an instance of PIL.Image.\n",
    "    \"\"\"\n",
    "\n",
    "    img_copy = img.copy()\n",
    "    draw = ImageDraw.Draw(img_copy)\n",
    "\n",
    "    # Draw Bounding boxes\n",
    "    for b in bounding_boxes:\n",
    "        draw.rectangle([\n",
    "            (b[0], b[1]), (b[2], b[3])\n",
    "        ], outline='white')\n",
    "\n",
    "    # Draw Facial Landmarks\n",
    "    for p in facial_landmarks:\n",
    "        for i in range(5):\n",
    "            draw.ellipse([\n",
    "                (p[i] - 1.0, p[i + 5] - 1.0),\n",
    "                (p[i] + 1.0, p[i + 5] + 1.0)\n",
    "            ], outline='blue')\n",
    "\n",
    "    return img_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('miscs/m0807/image/office1.jpg')\n",
    "\n",
    "bounding_boxes, landmarks = detect_faces(img)\n",
    "show_bboxes(img, bounding_boxes, landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('miscs/m0807/image/office2.jpg')\n",
    "\n",
    "bounding_boxes, landmarks = detect_faces(img)\n",
    "show_bboxes(img, bounding_boxes, landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('miscs/m0807/image/office3.jpg')\n",
    "\n",
    "bounding_boxes, landmarks = detect_faces(img)\n",
    "show_bboxes(img, bounding_boxes, landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('miscs/m0807/image/office4.jpg')\n",
    "\n",
    "bounding_boxes, landmarks = detect_faces(img)\n",
    "show_bboxes(img, bounding_boxes, landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('miscs/m0807/image/office5.jpg')\n",
    "\n",
    "bounding_boxes, landmarks = detect_faces(img)\n",
    "show_bboxes(img, bounding_boxes, landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('miscs/m0807/image/image1.jpg')\n",
    "\n",
    "bounding_boxes, landmarks = detect_faces(img)\n",
    "show_bboxes(img, bounding_boxes, landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('miscs/m0807/image/image2.jpg')\n",
    "\n",
    "bounding_boxes, landmarks = detect_faces(img)\n",
    "show_bboxes(img, bounding_boxes, landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('miscs/m0807/image/image3.jpg')\n",
    "\n",
    "bounding_boxes, landmarks = detect_faces(img)\n",
    "show_bboxes(img, bounding_boxes, landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('miscs/m0807/image/image4.jpg')\n",
    "\n",
    "bounding_boxes, landmarks = detect_faces(img)\n",
    "show_bboxes(img, bounding_boxes, landmarks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
