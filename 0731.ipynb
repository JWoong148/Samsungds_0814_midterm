{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.compat.v1.distributions import Normal, kl_divergence\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "from scipy.special import logsumexp\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayes Dense Layer\n",
    "\n",
    "def bayes_dense(x, num_units, name='dense', gamma=1.0, activation=None):\n",
    "    with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
    "        W_mu = tf.get_variable('W_mu', [x.shape[1], num_units])\n",
    "        W_rho = tf.nn.softplus(\n",
    "                tf.get_variable('W_rho', [x.shape[1], num_units],\n",
    "                    initializer=tf.random_uniform_initializer(-3., -2.)))\n",
    "        b_mu = tf.get_variable('b_mu', [num_units],\n",
    "                initializer=tf.zeros_initializer())\n",
    "        b_rho = tf.nn.softplus(\n",
    "                tf.get_variable('b_rho', [num_units],\n",
    "                    initializer=tf.random_uniform_initializer(-3., -2.)))\n",
    "\n",
    "    xW_mean = tf.matmul(x, W_mu)\n",
    "    xW_std = tf.sqrt(tf.matmul(tf.square(x), tf.square(W_rho)) + 1e-6)\n",
    "    xW = xW_mean + xW_std*tf.random.normal(tf.shape(xW_mean))\n",
    "    b = b_mu + b_rho * tf.random.normal(b_mu.shape)\n",
    "\n",
    "    x = xW + b\n",
    "    if activation == 'relu':\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "    # kl divergence\n",
    "    kld_W = tf.reduce_sum(kl_divergence(Normal(W_mu, W_rho), Normal(0., gamma)))\n",
    "    kld_b = tf.reduce_sum(kl_divergence(Normal(b_mu, b_rho), Normal(0., gamma)))\n",
    "    kld = kld_W + kld_b\n",
    "\n",
    "    return x, kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "\n",
    "def func(x, noise):\n",
    "    eps = noise * np.random.randn(*x.shape)\n",
    "    return x + eps + np.sin(4*(x + eps)) + np.sin(13*(x + eps))\n",
    "\n",
    "num_train = 50\n",
    "num_test = 20\n",
    "np.random.seed(42)\n",
    "x_train_np = np.concatenate([\n",
    "    0.5 * np.random.rand(num_train//2, 1),\n",
    "    0.8 + 0.2*np.random.rand(num_train//2, 1)], 0)\n",
    "y_train_np = func(x_train_np, 0.03)\n",
    "x_test_np = np.random.rand(num_test, 1)\n",
    "y_test_np = func(x_test_np, 0.03)\n",
    "np.random.seed(int(time.time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1, 2: Fill in the missing parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "kl_coeff = 0.01\n",
    "num_steps = 10000\n",
    "num_samples = 50\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "# define network\n",
    "# BayesDense(784, 100) - ReLU\n",
    "# BayesDense(100, 100) - ReLU\n",
    "# BayesDense(100, 2)\n",
    "#### Problem 1 ####\n",
    "out, kld1 = \n",
    "out, kld2 = \n",
    "out, kld3 = \n",
    "###################\n",
    "mu, sigma = tf.split(out, 2, axis=-1)\n",
    "sigma = tf.nn.softplus(sigma)\n",
    "\n",
    "# log-likelihood\n",
    "ll = tf.reduce_mean(Normal(mu, sigma).log_prob(y))\n",
    "# kl-divergence\n",
    "kld = kl_coeff * (kld1 + kld2 + kld3) / np.float32(num_train)\n",
    "\n",
    "lr = tf.placeholder(tf.float32)\n",
    "train_op = tf.train.AdamOptimizer(lr).minimize(#### Problem 2 ####)\n",
    "\n",
    "def get_lr(t):\n",
    "    if t < 0.25 * num_steps:\n",
    "        return 0.01\n",
    "    elif t < 0.5 * num_steps:\n",
    "        return 0.1 * 0.01\n",
    "    else:\n",
    "        return 0.01 * 0.01\n",
    "\n",
    "# training loop\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for t in range(num_steps):\n",
    "    \n",
    "    sess.run(train_op, {x:x_train_np, y:y_train_np, lr:get_lr(t)})\n",
    "\n",
    "    if (t+1)% 100 == 0:\n",
    "        train_ll, train_kld = sess.run([ll, kld], {x:x_train_np, y:y_train_np})\n",
    "        print('step %d train ll %.4f train kld %.4f' % (t+1, train_ll, train_kld))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You can check your code by running the below cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test & Visualization\n",
    "\n",
    "x_np = np.linspace(0.0, 1.0, 100)[...,None]\n",
    "y_true_np = func(x_np, 0.0)\n",
    "\n",
    "mu_np_list, sigma_np_list = [], []\n",
    "ll_np_list = []\n",
    "for i in range(num_samples):\n",
    "    mu_np, sigma_np = sess.run([mu, sigma], {x:x_np})\n",
    "    mu_np, sigma_np = mu_np.squeeze(), sigma_np.squeeze()\n",
    "    ll_np = sess.run(Normal(mu, sigma).log_prob(y), {x:x_test_np, y:y_test_np})\n",
    "    mu_np_list.append(mu_np)\n",
    "    sigma_np_list.append(sigma_np)\n",
    "    ll_np_list.append(ll_np)\n",
    "\n",
    "ll_np_list = np.stack(ll_np_list, 0).squeeze(-1)\n",
    "print('test ll: %.4f' % (logsumexp(ll_np_list, 0) - np.log(num_samples)).mean())\n",
    "\n",
    "x_np = x_np.squeeze()\n",
    "y_true_np = y_true_np.squeeze()\n",
    "x_train_np = x_train_np.squeeze()\n",
    "y_train_np = y_train_np.squeeze()\n",
    "x_test_np = x_test_np.squeeze()\n",
    "y_test_np = y_test_np.squeeze()\n",
    "\n",
    "plt.figure()\n",
    "for i in range(num_samples):\n",
    "    upper = mu_np_list[i].squeeze() + sigma_np_list[i].squeeze()\n",
    "    lower = mu_np_list[i].squeeze() - sigma_np_list[i].squeeze()\n",
    "    plt.fill_between(x_np, lower, upper, alpha=0.01, color='skyblue')\n",
    "    plt.plot(x_np, mu_np_list[i].squeeze(), color='blue', alpha=0.05)\n",
    "\n",
    "plt.plot(x_train_np, y_train_np, 'r.', label='train')\n",
    "plt.plot(x_test_np, y_test_np, 'b*', label='test')\n",
    "plt.plot(x_np, y_true_np, 'k--', label='true')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
